paths:
  data: data/historical_btc.parquet
  checkpoints_dir: models/checkpoints
  best_policy: models/checkpoints/best_policy.pt
  best_rl_policy: models/checkpoints/best_rl_policy.pt
  log_dir: logs
  replay_buffer: data/replay_buffer.parquet

data_source:
  exchange_id: bitstamp       # binance | bybit | okx ...
  fallback_exchanges: []   # optional list for retries
  symbol: BTC/USDT         # ccxt symbol format
  timeframe: 1m
  start_iso: 2024-01-01T00:00:00Z
  max_bars: 500000
  auto_download_on_missing: true
  allow_synthetic_fallback: false
  train_start: "2019-01-01"
  train_end:   "2021-12-31"
  val_start:   "2022-01-01"
  val_end:     "2022-12-31"
  test_start:  "2023-01-01"
  test_end:    "2024-12-31"

cost:
  commission_bps: 0.25
  slippage_bps: 0.5
  spread_bps: 1.0

model:
  input_dim: 15        # inferred from data/features when training or first live candle
  hidden_sizes: [256, 256, 128]
  activation: relu
  action_space: discrete  # discrete|continuous

offline_training:
  epochs: 50
  batch_size: 1024
  learning_rate: 0.0003
  weight_decay: 0.00005
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  seed: 42

online_training:
  learning_rate: 0.00001
  weight_decay: 0.0
  batch_size: 64
  update_every: 100
  max_buffer_size: 200000
  weight_anchor_coef: 0.01

paper_trading:
  fee_taker: 0.0004
  fee_maker: 0.0002
  starting_balance: 10000
  max_position_size: 1.0     # fraction of equity
  slippage_bps: 1.0

live_feed:
  source: binance           # binance|simulate
  symbol: BTCUSDT
  interval: 1m
  rest_poll_seconds: 60
  websocket_url: wss://fstream.binance.com/ws

llm:
  model: gpt-4.1-mini
  temperature: 0.2
  max_tokens: 400
  enable: false

strategy:
  type: rl                  # rl | rule_based
  rule_based:
    fast_idx: 0             # feature index for fast signal (e.g., short EMA/logret)
    slow_idx: 1             # feature index for slow signal
    threshold: 0.0

execution:
  mode: paper               # paper | binance_testnet | binance_live
  symbol: BTCUSDT
  leverage: 1

risk:
  max_daily_loss_pct: 0.05
  max_leverage: 2.0
  max_position_notional_pct: 1.0
  cooldown_minutes: 60
  drawdown_stop_pct: 0.15

rl:
  algo: ppo
  device: cuda
  total_timesteps: 5000000
  rollout_length: 4096
  batch_size: 512
  gamma: 0.995
  gae_lambda: 0.95
  clip_ratio: 0.15
  actor_lr: 0.0001
  critic_lr: 0.0004
  value_coef: 0.5
  entropy_coef: 0.02
  min_action_prob: 0.05      # exploration floor for sampling
  flat_penalty: -0.007       # stronger opportunity cost for staying flat
  max_grad_norm: 0.5
  reward_scale: 1.0
  action_mapping: [-1.0, -0.5, 0.0, 0.5, 1.0]
  episode_length: 2048
  min_start_index: 0
  max_start_index: 480000
  excess_return_alpha: 0.7    # lower benchmark weight so exposure can be neutral/positive
  dd_penalty_coeff: 0.01
  dd_threshold: 0.3
  vol_penalty_coeff: 0.0
  vol_window: 50
  turnover_penalty_coeff: 0.0001
  exposure_penalty_coeff: 0.0   # reduce bias against taking exposure
  max_steps_per_episode: 10000
  initial_equity: 1000.0
  fee_rate: 0.0004
  slippage: 0.0002
  min_equity: 1.0

live_risk:
  max_daily_dd: 0.1
  max_position_notional: 2.0
  max_consecutive_losses: 10
